\subsection{Cross entropy error measure}


We start by defining $X$ to be a sample space and $Y$ be the label space of \{-1,1\}.
We want to learn the conditional probability $P(y|x)$ for $y \in {-1,1}$ and all $x \in X$.
But we have to be able to parametrize $P(y|x)$ by $w$, which we choose from a parameter space so that $w \in W$.
This way we get the value of $P_w(y|x)$ for $y \in -1,1$ and all $x \in X$.

Now we need a function to choose some parameter $\hat{w} \in W$ and therefore a corresponding distribution $P_{\hat{w}}(y|x)$ as well.
The only information on which we can make these assumptions is the labeled sample $S = \{(x_1,y_1),...,(x_N,y_N)\}$. 

I use the maximum likelihood function for that with the parameter $w$ as described above with the given sample $S$:
\begin{align}
L_S(w) = \prod_{(x_n,y_n)\in \mathcal{S}}  P_w(y_n|x_n)
= \prod_{n=1}^N P_w(y_n|x_n)
\end{align}
and then saying that we should choose $\hat{w} \in \mathcal{W}$ such that $L_\mathcal{S}$ is maximized.

Since the function $-\ln$ is monotonically decreasing, this strategy is equivalent\footnote{I define two strategies to be equivalent, if and only if they end up choosing the same $\hat{w}$ for all possible samples $S = \{(x_1,y_1),...,(x_N,y_N)\}$.} to choosing $\hat{w} \in \mathcal{W}$ such that the function
\begin{align}
f_S(w) = -\ln \left( \prod_{n=1}^N P_w(y_n|x_n) \right) = \sum_{n=1}^N \left( - \ln P_w(y_n|x_n) \right)
\end{align}
is minimized. 

Since $y \in \{-1,1\}$, then we can write $P_w(y|x)$ as
\begin{align}
P_w(y|x)  = \\ 
[[y = 1]] P_w(y = 1|x) + [[y = 1]] P_w(y = -1|x) = \\ 
[[y = 1]] h_w(x) + [[y = 1]] (1 - h_w(x))
\end{align}
where we simply have defined $h_w(x) = P(y = 1 | x)$. We therefore have that
\begin{align}
- \ln P_w(y_n|x_n) = \\ 
-\ln ([[y = 1]] h_w(x) + [[y = -1]] (1 - h_w(x))) = \\ 
[[y = 1]](-\ln (h_w(x)) + [[y = -1]](-\ln (1 - h_w(x))) = \\ 
[[y = 1]]\left(\ln \left( \frac{1}{h_w(x)}\right)\right) + [[y = -1]]\left(\ln \left( \frac{1}{1 - h_w(x)}\right)\right)
\end{align}

By line (2) and line (6-9), we now get that
\begin{align}
f_S(w) = \sum_{n=1}^N \left[ [[y_n = 1]]\left(\ln \left( \frac{1}{h_w(x_n)}\right)\right) + [[y_n = -1]]\left(\ln \left( \frac{1}{1 - h_w(x_n)}\right)\right) \right]
\end{align}
As I have already said, we will end up with the same $\hat{w}$ for a given sample $S$, if we minimize $f_S(w)$ as if we maximize $L_S(w)$. If we had started by saying that we would like to estimate the probability $h_w(x) = P_w(y = 1|x)$ by choosing $\hat{w}$ such that we minimize the error function $f_S(w)$ defined as in line (10), we would have ended up with the same estimates of $P(y|x)$ for $y \in \{-1,1\}$ and $x\in \mathcal{X}$, as if we have used the maximum likelihood method. These two strategies are therefore equivalent.

If we now focus on logistic regression and set $h_w(x) = \Theta(w^T x) = \frac{e^{w^Tx}}{1 + e^{w^Tx}}$, we know from the book that $1 - h_w(x) = h_w(-x)$, which means that
\begin{align}
f_S(w) = \sum_{n=1}^N \left[ [[y_n = 1]]\left(\ln \left( \frac{1}{h_w(x_n)}\right)\right) + [[y_n = -1]]\left(\ln \left( \frac{1}{h_w(-x_n)}\right)\right) \right] = \\
 \sum_{n=1}^N  \ln \left( \frac{1}{h_w(y_nx_n)}\right)  = \sum_{n=1}^N  \ln \left( \frac{1}{\frac{e^{y_nw^Tx}}{1 + e^{y_ nw^Tx}}}\right)  = \sum_{n=1}^N  \ln \left( \frac{1 + e^{y_ nw^Tx}}{e^{y_ nw^Tx}}\right)  = \\
 \sum_{n=1}^N  \ln \left( 1 +e^{-y_ nw^Tx}\right)
\end{align}
Minimizing this function gives the same $w$ as minimizing:
\begin{align}
\frac{1}{N}f_S(w) = \frac{1}{N} \sum_{n=1}^N  \ln \left( 1 +e^{-y_ nw^Tx}\right)
\end{align}
This is exactly the function that the book says in statement (3.9) that we should minimize, when we search for the weights in logistic regression.


\subsection{Logistic regression loss gradient}

In the algorithm for logistic regression, we determine the parameter $w \in \mathbb{R}^m$ in our final hypothesis
\begin{align}
h_w(x) = P_w(y = 1|x) = \frac{e^{w^Tx}}{1 + e^{w^Tx}}
\end{align}
using a given labeled sample $S = \{(x_1,y_1),...,(x_N,y_N)\}$ by minizing the function
\begin{align}
E_{in}(w) = \frac{1}{N} \sum_{n=1}^N \ln(1 + e^{-y_n w^T x_n})
\end{align}
We use gradient descent to do this, which means we have to find the loss gradient $\nabla E_{in}(w)$. We can use matrix calculus to to differentiate $E_{in}$
\begin{align}
D_w (E_{in}(w)) = D_w\left( \frac{1}{N} \sum_{n=1}^N \ln(1 + e^{-y_n w^T x_n}) \right) = \\ 
 \frac{1}{N} \sum_{n=1}^N D_w \left( \ln(1 + e^{-y_n w^T x_n}) \right). 
\end{align}
Let us now focus on the inside of the sum
\begin{align}
D_w \left( \ln(1 + e^{-y_n w^T x_n}) \right) = \frac{D_w ( 1 + e^{-y_n w^T x_n})}{1 + e^{-y_n w^T x_n}} = \\ 
\frac{D_w (e^{-y_n w^T x_n})}{1 + e^{-y_n w^T x_n}} =
\frac{e^{-y_n w^T x_n} D_w(-y_nw^Tx)}{1 + e^{-y_n w^T x_n}} = \\ 
-y_n x_n \frac{e^{-y_n w^T x_n}}{1 + e^{-y_n w^T x_n}} = -y_n x_n \Theta(-y_n w^T x_n)
\end{align}
Line (13-14) and line (15-17) now gives us that
\begin{align}
D_w (E_{in}(w)) =  \frac{1}{N} \sum_{n=1}^N D_w \left( \ln(1 + e^{-y_n w^T x_n}) \right) =
\frac{1}{N} \sum_{n=1}^N  -y_n x_n \Theta(-y_n w^T x_n) 
\end{align}



\subsection{Logistic regression implementation}


\subsection{Iris Flower Data}
