%Question 1
\subsection{Question 1}
The hypothesis space $\mathcal{H} = \{h_1,...,h_M \}$ is finite in this question, which means $|\mathcal{H}| = M$ for which we can use \textbf{Theorem 3.2} from which we conclude with probability $1 - \delta$ for all $h \in \mathcal{H}$
\begin{align}
L(\hat{h}^*) \leq \hat{L}(\hat{h}^*, S_{val}) + \sqrt{\frac{\ln \frac{M}{\delta}}{2n}}
\end{align}
where $n$ is equal to $|S_{val}|$, when we insert that we end up in the following:
\begin{align}
L(\hat{h}^*) \leq \hat{L}(\hat{h}^*, S_{val}) + \sqrt{\frac{\ln \frac{M}{\delta}}{2|s_{val}|}}
\end{align}

%% Question 2
\subsection{Question 2}
So first let $S_{val}^*$ be the validation set with which we are testing the hypothesis $\hat{h}^*$ on which we pick.
According to our fellow student we test a single hypothesis $\hat{h}^*$ on $S_{val}^*$ and the splitting of the validation set described as $|S_{val^*}|=\frac{n}{M}$.
So we can use \textbf{Theorem 3.1} to conclude with probability $1 - \delta$ for all $h \in \mathcal{H}$ 
\begin{align}
L(\hat{h}^*) \leq \hat{L}(\hat{h}^*, S_{val}) + \sqrt{\frac{\ln \frac{1}{\delta}}{2\frac{n}{M}}} = \hat{L}(\hat{h}^*, S_{val}) + \sqrt{\frac{ M \ln \frac{1}{\delta}}{2n}}
\end{align}
No it wasn't a good idea, since the bound is growing linear with $M$ instead of logarithmically.

\subsection{Question 3}


\textbf{a)} Again we only test a single hypothesis, namely $\hat{h}^*$, on $S_{val}^2$. This time we have that $|S_{val}^2|=\frac{n}{2}$. Therefore, we can use \textbf{Theorem 3.1} to conclude that with probability $1 - \delta$ for all $h \in \mathcal{H}$
\begin{align}
L(\hat{h}^*) \leq \hat{L}(\hat{h}^*, S_{val}) + \sqrt{\frac{\ln \frac{1}{\delta}}{2\frac{n}{2}}} = \hat{L}(\hat{h}^*, S_{val}) + \sqrt{\frac{ \ln \frac{1}{\delta}}{n}}
\end{align}
\textbf{b)} 
Since we have my hypothesis and the hypothesis of my fellow student, let $\hat{h}^*$ be my hypothesis which I choose and $\bar{h}^*$ for the fellow student.
I will use the full $S_{val}$ to choose my hypothesis $\hat{h}^*$ while the fellow student uses $S^1_{val}$ to choose $\bar{h}^*$
From the assignment text we then assume:
$\hat{L}(\hat{h}^*,S_{val}) = \hat{L}(\bar{h}^*,S_{val}^2)$, from that we know that my bound will be tighter, if:
\begin{align}
\sqrt{\frac{\ln \frac{M}{\delta}}{2n}} < \sqrt{\frac{ \ln \frac{1}{\delta}}{n}}\\
= \ln \frac{M}{\delta} < 2 \ln \frac{1}{\delta}\\
= \frac{M}{\delta} < \left( \frac{1}{\delta} \right)^2\\
= M \delta < 1
\end{align}
No we can say as an example that we want to have a certainty of $1-\delta = 0.90$, then my bound would only be tighter for $M < 10$.\\\\
\textbf{c)} Since I use a a bigger validation set then my fellow student I would have the higher probability of choosing $h_i$ in $\mathcal{H}$ with lowest expected loss $L(h_i)$, even if $M$ is large. Which I would see as a drawback of the chosen method.
\subsection{Question 4}
\textbf{a)} By choosing a large $\alpha$ we would have the advantage of also having a large validation set $S_{val}^1$, which means a better chance of choosing the hypothesis in $\mathcal{H}$ with the lowest expected loss. This also means we should expect a lower empirical loss on the test set $S_{val}^2$.
The downside is that with a large $\alpha$ also the test set gets smaller, so we got more uncertain how well the empirical loss reflect the true expected loss.
By choosing a smaller $\alpha$ we end up in similar issues as described for a bigger $\alpha$ but the other way around, lower chance of choosing the hypothesis in $\mathcal{H}$ with the lowest expected loss.\\
\textbf{b)} I would make this selection according to the size of $M$, so when $M$ becomes larger I would also choose $\alpha$ larger.
Since the larger the hypothesis space gets, this also means a bigger probability of choosing a bad hypothesis $\hat{h}^*$.